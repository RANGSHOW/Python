{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 전체 구조 및 학습 과정\n",
    "\n",
    "### Data >>> [[ Model >>> logit >>> Loss >>> Optm ]] x n >>> Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- #### 학습시키기 위한 데이터. 이 데이터가 모델에 입력됨\n",
    "- #### 데이터가 생성되고, 데이터에 Transform 변형을 준다거나 모델에 들어가기 전에 데이터 전처리가 들어감\n",
    "    - ##### Data Argmentation: 데이터 전처리의 방법 중 하나로, 정보량을 유지한 상태로 데이터에 노이즈를 주는 것\n",
    "- #### 이 때 들어갈 때는 Batch로 만들어서 Model에 넣어줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- #### CNN (Convolutional Neural Network)\n",
    "- #### LeNet, AlexNet, VGG 나 ResNet 등 다양하게 설계된 모델\n",
    "- #### Convolution Layer, Pooling 등 다양한 Layer층들로 구성\n",
    "- #### 이 모델 안에 학습 파라미터가 있고, 이 모델이 학습하는 대상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction / Logit\n",
    "### [ 0.15, 0.3, 0.2, 0.25, 0.1 ]\n",
    "- #### 각 Class별로 예측한 값.\n",
    "- #### 여기서 가장 높은 값이 모델이 예상하는 class 또는 정답\n",
    "\n",
    "### [ 0.0, 0.0, 0.0, 1.0, 0.0 ]\n",
    "- #### 위의 숫자가 정답이라고 할 때 얼마나 틀렸는지 얼마나 맞았는지 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss / Cost\n",
    "- #### 예측한 값과 정답과 비교해서 얼마나 틀렸는지를 확인.\n",
    "- #### Cross Entropy 등 다양한 Loss Function들이 있음\n",
    "- #### 이 때 계산을 통해 나오는 값이 Loss(Cost, Cost Value 등)이라고 불림\n",
    "- #### 이 Loss는 \"얼마나 틀렸는지\"를 말하며 이 값을 최대한 줄이는 것이 학습의 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- #### 앞에서 얻은 Loss 값을 최소한하기 위해 기울기를 받아 최적화된 Variable 값들로 반환\n",
    "- #### 이 반환된 값이 적용된 모델은 바로 전에 돌렸을 때의 결과보다 더 나아지게 됨\n",
    "- #### 이 때 바로 최적화된 값만큼 바로 움직이는 것이 아닌 Learning Rate 만큼 움직인 값이 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "- #### 평가 할 때 또는 예측된 결과를 확인 할 때에는 예측된 값에서 argmax를 통해 가장 높은 값을 예측한 class 라고 둠\n",
    "### [ 0.15, 0.3, 0.2, 0.25, 0.1 ]\n",
    "- #### 위의 예측값에서는 0.2가 제일 높은 값이므로 클래스 2가 가장 높다고 봄 (파이썬에선 0 으로 시작)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning 용어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "- #### DeepLearning 이라는 게 Layer를 여러 겹 쌓았기 때문에 붙여진 이름이다.\n",
    "- #### < Input Layer > --- <<< Hidden Layers >>> --- < Output Layer > 로 구성되어 있다\n",
    "- #### Hidden Layer를 쌓는 방식에 따라 모델이 달라진다\n",
    "- #### 예를 들어 'VGG16'은 VGG Model을 16개의 Layer로 구성한 것이다\n",
    "    - ##### 깊이 쌓으면 Feature을 Detail하게 뽑을 수 있다\n",
    "    - ##### 층이 많으면 무조건 좋은 것은 아니다. 모델이 무거워지고, 과적합(Overfitting)이 일어날 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "- #### 합성곱\n",
    "- #### Input Image에 Convolution Kernel(filter)을 곱하여 Feature Map을 얻는 과정\n",
    "- #### 가령 Convolution Kernel이 [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]] 라면 테두리가 Feature Map에 투사된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight / Filter / Kernel / Variable / Bias\n",
    "- ### Weight\n",
    "    - #### 학습하고자 하는 대상\n",
    "    - #### Convolution Kernel은 고정된 값이 아니라 더 좋은 Feature Map을 만들기 위해 성능을 높여나간다\n",
    "    - #### 데이터의 특성과 목적에 따라 Weight를 변화시켜가며 학습시켜야 한다\n",
    "    - #### y = Wx + B (W: weight, B: bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "- #### Convolved Feature을 Pooled feature로 압축시켜준다\n",
    "- #### Max Pooling\n",
    "    - ##### 이미지가 가지고 있는 가장 큰 특징들(높은 수치들)을 반으로 줄여준다\n",
    "- #### Convolution은 특징을 뽑은 Layer층\n",
    "- #### Pooling Layer은 압축을 하는 Layer층이라고 생각하면 편하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "- #### 앞에서 뽑은 특징을 음수값이나 불필요한 값을 제거하기도 한다\n",
    "- ### Activation Functions\n",
    "    - #### Sigmoid (omega(x) = 1 / 1 + e^(-x))\n",
    "    - #### Leaky ReLU (max(0.1x, x))\n",
    "    - #### tanh\n",
    "    - #### Maxout (max(w_1 ^T x + b_1, w_2 ^T x + b_2))\n",
    "    - #### ReLU (max(0, x))\n",
    "    - #### ELU ( x (x >= 0) , alpha(e^x - 1) (x < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "- #### 최종 단계에서 사용되는 합수\n",
    "- #### 앞에서 받은 값들을 확률로 나타내주는 역할이다 (모든 Class의 확률 합 = 1)\n",
    "- #### Class들의 Logits scores를 Softmax를 통해 Probabilities로 바꿔준다\n",
    "- ### S(y_i) = e^(y_i) / Sum(e^(y_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- #### 인공지능을 학습시키고 난 후, 최적화 시키는 과정이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
